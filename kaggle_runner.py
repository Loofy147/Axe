"""
This script is a self-contained, reproducible pipeline for training a robust
futures-axis classification model.

It addresses the issue of "keyword matching" by incorporating:
1.  **Automated Data Augmentation:** If an augmented dataset is not found,
    it is generated by replacing keywords with synonyms using NLTK and WordNet.
    This creates a more challenging dataset that discourages simple
    keyword memorization.
2.  **Self-Supervised Learning:** The model is trained on a combined objective
    of axis classification and Masked Language Modeling (MLM). The MLM task
    forces the model to learn deeper contextual relationships between words,
    improving its semantic understanding.
3.  **Keyword Robustness Analysis:** After training, the model is explicitly
    tested on its reliance on keywords. This analysis provides a quantitative
    measure of the model's conceptual understanding.

The final result is a model that is not only accurate but also more robust
and less susceptible to the biases of the original dataset.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
import json
from collections import Counter
import pickle
import torch.optim as optim
import os
import random
from nltk.corpus import wordnet

# --- Centralized Configuration ---

KEYWORDS_BY_AXIS = {
    0: ["ai", "robotic", "automated", "automation"],
    1: ["human", "centric", "personalized"],
    2: ["resource", "abundant", "materials"],
    3: ["individualistic", "personal", "independent"],
    4: ["community", "collective", "shared"],
    5: ["global", "interconnected", "worldwide"],
    6: ["crisis", "disaster", "collapse"],
    7: ["restoration", "rebuild", "rehabilitate"],
    8: ["adaptation", "resilience", "cope"],
    9: ["immersive", "digital", "virtual"],
    10: ["physical", "tangible", "hands-on"],
    11: ["collaborative", "co-creation", "teamwork"]
}

# --- Data Augmentation ---

def get_synonyms(word):
    """
    Get synonyms for a word using WordNet.
    """
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name().replace('_', ' '))
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def augment_dataset(input_path='futures_dataset.json', output_path='augmented_futures_dataset.json'):
    """
    Augments the dataset by replacing keywords with synonyms.
    """
    print(f"Augmenting dataset from '{input_path}'...")

    with open(input_path) as f:
        data = json.load(f)

    augmented_samples = []
    for sample in data['samples']:
        text = sample['text']
        axis_id = sample['axis_id']

        relevant_keywords = KEYWORDS_BY_AXIS.get(axis_id, [])

        # Augment by replacing one keyword at a time
        for kw in relevant_keywords:
            if kw in text.lower():
                synonyms = get_synonyms(kw)
                if synonyms:
                    synonym = random.choice(synonyms)
                    new_text = text.lower().replace(kw, synonym, 1)

                    augmented_samples.append({
                        'text': new_text.capitalize(),
                        'axis_id': axis_id
                    })

    # Combine original and augmented data
    combined_samples = data['samples'] + augmented_samples

    # Shuffle the combined dataset
    random.shuffle(combined_samples)

    output_data = {'samples': combined_samples}

    with open(output_path, 'w') as f:
        json.dump(output_data, f, indent=2)

    print(f"Augmented dataset saved to '{output_path}'.")
    print(f"  - Original samples: {len(data['samples'])}")
    print(f"  - Augmented samples added: {len(augmented_samples)}")
    print(f"  - Total samples: {len(combined_samples)}")

# --- Model and Training Components ---

class CustomTokenizer:
    def __init__(self, vocab_dict):
        self.vocab_dict = vocab_dict
        self.idx_to_word = {idx: word for word, idx in vocab_dict.items()}
        self.pad_token_id = vocab_dict['<PAD>']
        self.unk_token_id = vocab_dict['<UNK>']
        self.vocab_size = len(vocab_dict)

    def encode(self, text, max_length=50):
        text = text.lower()
        text = text.replace(',', ' ,').replace('.', ' .')
        text = text.replace('(', ' ( ').replace(')', ' ) ')
        text = text.replace('"', ' " ').replace("'", " ' ")
        words = text.split()

        token_ids = []
        for word in words:
            token_ids.append(self.vocab_dict.get(word, self.unk_token_id))

        if len(token_ids) < max_length:
            token_ids += [self.pad_token_id] * (max_length - len(token_ids))
        else:
            token_ids = token_ids[:max_length]

        return token_ids

    def decode(self, token_ids, skip_special_tokens=True):
        words = []
        special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']

        for idx in token_ids:
            if isinstance(idx, int):
                word = self.idx_to_word.get(idx, '<UNK>')
            else:
                word = self.idx_to_word.get(idx.item(), '<UNK>')

            if skip_special_tokens and word in special_tokens:
                continue
            words.append(word)

        text = ' '.join(words)
        text = text.replace(' ,', ',').replace(' .', '.')
        text = text.replace('( ', '(').replace(' )', ')')
        return text


class SimplifiedFuturesModel(nn.Module):
    """
    ENHANCED SIMPLE model for axis classification.
    """

    def __init__(self, vocab_size, n_classes=12, d_model=128, n_head=4, n_layers=2):
        super().__init__()

        self.d_model = d_model

        self.token_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Embedding(50, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_head,
            dim_feedforward=d_model * 4,
            batch_first=True,
            dropout=0.1
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

        self.classifier = nn.Linear(d_model, n_classes)
        self.mlm_head = nn.Linear(d_model, vocab_size)

    def forward(self, tokens):
        batch_size, seq_len = tokens.shape
        token_emb = self.token_emb(tokens)

        positions = torch.arange(seq_len, device=tokens.device).unsqueeze(0).expand(batch_size, -1)
        pos_emb = self.pos_emb(positions)

        x = token_emb + pos_emb

        transformer_output = self.transformer(x)

        mask = (tokens != self.token_emb.padding_idx).float() if self.token_emb.padding_idx is not None else (tokens != 0).float()
        x_masked = transformer_output * mask.unsqueeze(-1)
        x_pooled = x_masked.sum(dim=1) / mask.sum(dim=1, keepdim=True).clamp(min=1)
        classification_logits = self.classifier(x_pooled)

        mlm_logits = self.mlm_head(transformer_output)

        return classification_logits, mlm_logits


def build_custom_vocabulary(dataset_path, vocab_size=3000):
    print("Building vocabulary...")
    with open(dataset_path) as f:
        data = json.load(f)

    word_counts = Counter()
    for sample in data['samples']:
        text = sample['text'].lower()
        text = text.replace(',', ' ,').replace('.', ' .')
        text = text.replace('(', ' ( ').replace(')', ' ) ')
        words = text.split()
        word_counts.update(words)

    special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>', '<MASK>']
    top_words = [word for word, _ in word_counts.most_common(vocab_size)]
    vocabulary = special_tokens + top_words

    vocab_dict = {word: idx for idx, word in enumerate(vocabulary)}

    with open('custom_vocab_simple.pkl', 'wb') as f:
        pickle.dump({'vocab_dict': vocab_dict}, f)

    print(f"Vocabulary size: {len(vocab_dict)}")
    return vocab_dict


class CustomVocabDataset(Dataset):
    def __init__(self, json_path, tokenizer, max_length=50, mlm_probability=0.15):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.mlm_probability = mlm_probability
        self.mask_token_id = tokenizer.vocab_dict['<MASK>']
        self.pad_token_id = tokenizer.pad_token_id
        self.unk_token_id = tokenizer.unk_token_id

        with open(json_path) as f:
            data = json.load(f)

        self.samples = []
        for item in data['samples']:
            tokens = tokenizer.encode(item['text'], max_length=max_length)
            self.samples.append({
                'tokens': torch.LongTensor(tokens),
                'axis_id': item['axis_id']
            })

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        tokens = sample['tokens'].clone()
        axis_id = sample['axis_id']

        mlm_labels = tokens.clone()

        special_token_ids = {self.pad_token_id, self.unk_token_id}
        can_be_masked = torch.tensor([t not in special_token_ids for t in tokens])

        probability_matrix = torch.full(mlm_labels.shape, self.mlm_probability)
        masked_indices = torch.bernoulli(probability_matrix).bool() & can_be_masked

        if not masked_indices.any() and can_be_masked.any():
             maskable_indices = torch.where(can_be_masked)[0]
             chosen_index = maskable_indices[torch.randint(0, len(maskable_indices), (1,))]
             masked_indices[chosen_index] = True

        mlm_labels[~masked_indices] = -100

        indices_replaced = torch.bernoulli(torch.full(mlm_labels.shape, 0.8)).bool() & masked_indices
        tokens[indices_replaced] = self.mask_token_id

        indices_random = torch.bernoulli(torch.full(mlm_labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(0, self.tokenizer.vocab_size, mlm_labels.shape, dtype=torch.long)
        tokens[indices_random] = random_words[indices_random]

        return tokens, mlm_labels, axis_id


def train_and_evaluate(
    dataset_path='augmented_futures_dataset.json',
    num_epochs=50,
    batch_size=32,
    learning_rate=1e-4,
    mlm_weight=1.0
):
    print("=" * 80)
    print("ENHANCED MODEL TRAINING (CLASSIFICATION + MLM)")
    print("=" * 80)

    vocab_dict = build_custom_vocabulary(dataset_path)
    tokenizer = CustomTokenizer(vocab_dict)
    vocab_size = len(vocab_dict)

    full_dataset = CustomVocabDataset(dataset_path, tokenizer)
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    print(f"Train samples: {len(train_dataset)}")
    print(f"Val samples: {len(val_dataset)}")

    model = SimplifiedFuturesModel(
        vocab_size=vocab_size,
        n_classes=12,
        d_model=128,
        n_head=4,
        n_layers=2
    )

    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    criterion_class = nn.CrossEntropyLoss()
    criterion_mlm = nn.CrossEntropyLoss(ignore_index=-100)
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

    best_val_acc = 0
    patience = 10
    patience_counter = 0

    print("Training...")

    for epoch in range(num_epochs):
        model.train()
        total_train_loss = 0
        train_correct = 0
        train_total = 0
        total_mlm_loss = 0

        for masked_tokens, mlm_labels, axis_ids in train_loader:
            optimizer.zero_grad()

            class_logits, mlm_logits = model(masked_tokens)

            loss_class = criterion_class(class_logits, axis_ids)
            loss_mlm = criterion_mlm(mlm_logits.view(-1, vocab_size), mlm_labels.view(-1))

            loss = loss_class + mlm_weight * loss_mlm

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_train_loss += loss.item()
            total_mlm_loss += loss_mlm.item()
            train_correct += (class_logits.argmax(dim=1) == axis_ids).sum().item()
            train_total += axis_ids.size(0)

        train_acc = 100 * train_correct / train_total
        avg_mlm_loss = total_mlm_loss / len(train_loader)

        model.eval()
        val_correct = 0
        val_total = 0
        total_val_mlm_loss = 0

        with torch.no_grad():
            for masked_tokens, mlm_labels, axis_ids in val_loader:
                class_logits, mlm_logits = model(masked_tokens)

                loss_mlm = criterion_mlm(mlm_logits.view(-1, vocab_size), mlm_labels.view(-1))
                total_val_mlm_loss += loss_mlm.item()

                val_correct += (class_logits.argmax(dim=1) == axis_ids).sum().item()
                val_total += axis_ids.size(0)

        val_acc = 100 * val_correct / val_total
        avg_val_mlm_loss = total_val_mlm_loss / len(val_loader)

        scheduler.step()

        print(f"Epoch {epoch+1:3d}/{num_epochs} | "
              f"Train Acc: {train_acc:5.2f}% | Val Acc: {val_acc:5.2f}% | "
              f"Train MLM Loss: {avg_mlm_loss:.3f} | Val MLM Loss: {avg_val_mlm_loss:.3f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0

            torch.save(model.state_dict(), 'checkpoint_best.pt')

            print(f"  → NEW BEST VAL ACC: {val_acc:.2f}%")
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break

    print("\nTRAINING COMPLETE")
    print(f"Best Validation Accuracy: {best_val_acc:.2f}%")

    # Load best model for analysis
    model.load_state_dict(torch.load('checkpoint_best.pt'))

    return model, tokenizer

def analyze_keyword_robustness(model, tokenizer, dataset_path='futures_dataset.json'):
    print("\n" + "="*80)
    print("Keyword Robustness Analysis")
    print("="*80)

    with open(dataset_path) as f:
        data = json.load(f)

    model.eval()
    total_samples = 0
    flips = 0

    for sample in data['samples']:
        text = sample['text']
        axis_id = sample['axis_id']

        relevant_keywords = KEYWORDS_BY_AXIS.get(axis_id, [])
        found_keyword = None
        for kw in relevant_keywords:
            if kw in text.lower():
                found_keyword = kw
                break

        if not found_keyword:
            continue

        total_samples += 1

        tokens_with_keyword = torch.LongTensor(tokenizer.encode(text)).unsqueeze(0)
        with torch.no_grad():
            logits_with, _ = model(tokens_with_keyword)
        pred_with = logits_with.argmax(dim=1).item()

        text_without_keyword = text.lower().replace(found_keyword, "")
        tokens_without_keyword = torch.LongTensor(tokenizer.encode(text_without_keyword)).unsqueeze(0)
        with torch.no_grad():
            logits_without, _ = model(tokens_without_keyword)
        pred_without = logits_without.argmax(dim=1).item()

        if pred_with != pred_without:
            flips += 1

    if total_samples > 0:
        flip_rate = 100 * flips / total_samples
        print(f"Robustness Score: {100 - flip_rate:.2f}%")
        print(f"  - Analyzed {total_samples} samples.")
        print(f"  - Prediction flipped in {flips} cases ({flip_rate:.2f}%).")

        if flip_rate < 15:
            print("✓✓✓ EXCELLENT: The model is highly robust.")
        elif flip_rate < 30:
            print("✓ GOOD: The model shows some robustness.")
        else:
            print("⚠️  POOR: The model is brittle.")
    else:
        print("Could not find any samples with keywords to analyze.")

if __name__ == "__main__":

    # Ensure NLTK data is available
    try:
        wordnet.synsets('test')
    except Exception:
        print("Downloading NLTK data...")
        import nltk
        nltk.download('wordnet')
        nltk.download('omw-1.4')

    # Augment dataset if it doesn't exist
    augmented_dataset_path = 'augmented_futures_dataset.json'
    if not os.path.exists(augmented_dataset_path):
        augment_dataset(output_path=augmented_dataset_path)

    # Train and evaluate the model
    model, tokenizer = train_and_evaluate(dataset_path=augmented_dataset_path)

    # Analyze robustness on the original, unmodified dataset
    if model and tokenizer:
        analyze_keyword_robustness(model, tokenizer, dataset_path='futures_dataset.json')
