{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14212072,"sourceType":"datasetVersion","datasetId":9065307}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nThis script consolidates the training and testing pipelines for the futures model,\ndesigned to be run in a Kaggle environment.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport json\nfrom pathlib import Path\nimport sys\nimport torch.nn.functional as F\nfrom transformers import GPT2Tokenizer\nfrom collections import Counter\nimport pickle\n\ndef build_custom_vocabulary(dataset_path='futures_dataset.json', vocab_size=3000):\n    \"\"\"\n    Build custom vocabulary from dataset.\n\n    Args:\n        dataset_path: Path to futures dataset JSON\n        vocab_size: Target vocabulary size (not including special tokens)\n\n    Returns:\n        vocab_dict: {word: index} mapping\n        idx_to_word: {index: word} mapping\n    \"\"\"\n\n    print(\"=\" * 70)\n    print(\"Building Custom Vocabulary\")\n    print(\"=\" * 70)\n\n    # Load dataset\n    print(f\"\\n1. Loading dataset from {dataset_path}...\")\n    with open(dataset_path) as f:\n        data = json.load(f)\n\n    samples = data['samples']\n    print(f\"   Total samples: {len(samples)}\")\n\n    # Tokenize and count words\n    print(\"\\n2. Counting words...\")\n    word_counts = Counter()\n\n    for sample in samples:\n        text = sample['text'].lower()\n\n        # Simple tokenization (split on whitespace and punctuation)\n        text = text.replace(',', ' ,').replace('.', ' .')\n        text = text.replace('(', ' ( ').replace(')', ' ) ')\n        text = text.replace('\"', ' \" ').replace(\"'\", \" ' \")\n\n        words = text.split()\n        word_counts.update(words)\n\n    total_words = sum(word_counts.values())\n    unique_words = len(word_counts)\n    print(f\"   Total words: {total_words:,}\")\n    print(f\"   Unique words: {unique_words:,}\")\n\n    # Create vocabulary\n    print(f\"\\n3. Creating vocabulary (target size: {vocab_size})...\")\n\n    # Special tokens\n    special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n\n    # Top N most common words\n    top_words = [word for word, _ in word_counts.most_common(vocab_size)]\n\n    # Combine\n    vocabulary = special_tokens + top_words\n    actual_vocab_size = len(vocabulary)\n\n    # Calculate coverage\n    top_word_count = sum(word_counts[w] for w in top_words)\n    coverage = 100 * top_word_count / total_words\n\n    print(f\"   Final vocabulary size: {actual_vocab_size:,}\")\n    print(f\"   Coverage: {coverage:.2f}% of dataset tokens\")\n\n    # Create mappings\n    vocab_dict = {word: idx for idx, word in enumerate(vocabulary)}\n    idx_to_word = {idx: word for word, idx in vocab_dict.items()}\n\n    # Save vocabulary\n    print(\"\\n4. Saving vocabulary...\")\n\n    with open('custom_vocab.json', 'w') as f:\n        json.dump(vocab_dict, f, indent=2)\n\n    with open('custom_vocab.pkl', 'wb') as f:\n        pickle.dump({\n            'vocab_dict': vocab_dict,\n            'idx_to_word': idx_to_word,\n            'word_counts': dict(word_counts.most_common(vocab_size)),\n            'special_tokens': special_tokens\n        }, f)\n\n    print(f\"   Saved to: custom_vocab.json and custom_vocab.pkl\")\n\n    # Statistics\n    print(\"\\n\" + \"=\" * 70)\n    print(\"Vocabulary Statistics\")\n    print(\"=\" * 70)\n    print(f\"Vocabulary size: {actual_vocab_size:,}\")\n    print(f\"Coverage: {coverage:.2f}%\")\n    print(f\"Unknown token rate: {100 - coverage:.2f}%\")\n\n    # Show top 20 words\n    print(\"\\nTop 20 most common words:\")\n    for i, (word, count) in enumerate(word_counts.most_common(20), 1):\n        print(f\"  {i:2d}. {word:20s} ({count:4d} occurrences)\")\n\n    # Show some axis-specific words\n    print(\"\\nSample vocabulary by category:\")\n    tech_words = [w for w in top_words if any(kw in w for kw in ['ai', 'robot', 'automat', 'algorithm', 'machine', 'digital'])][:10]\n    env_words = [w for w in top_words if any(kw in w for kw in ['climate', 'environment', 'carbon', 'energy', 'sustain', 'ecosystem'])][:10]\n    society_words = [w for w in top_words if any(kw in w for kw in ['community', 'individual', 'global', 'collective', 'society'])][:10]\n\n    if tech_words:\n        print(f\"\\n  Tech words: {', '.join(tech_words)}\")\n    if env_words:\n        print(f\"  Environment words: {', '.join(env_words)}\")\n    if society_words:\n        print(f\"  Society words: {', '.join(society_words)}\")\n\n    print(\"\\n\" + \"=\" * 70)\n    print(\"✓ Vocabulary creation complete!\")\n    print(\"=\" * 70)\n    print(\"\\nNext steps:\")\n    print(\"  1. Use custom_vocab.pkl in training\")\n    print(\"  2. Retrain model with vocab_size = \" + str(actual_vocab_size))\n    print(\"  3. Expected: Coherent text generation + 70-80% axis accuracy\")\n\n    return vocab_dict, idx_to_word\n\n\nclass CustomTokenizer:\n    \"\"\"Simple tokenizer using custom vocabulary\"\"\"\n\n    def __init__(self, vocab_dict):\n        self.vocab_dict = vocab_dict\n        self.idx_to_word = {idx: word for word, idx in vocab_dict.items()}\n        self.pad_token_id = vocab_dict['<PAD>']\n        self.unk_token_id = vocab_dict['<UNK>']\n        self.vocab_size = len(vocab_dict)\n\n    def encode(self, text, max_length=50):\n        \"\"\"Encode text to token IDs\"\"\"\n        # Simple tokenization\n        text = text.lower()\n        text = text.replace(',', ' ,').replace('.', ' .')\n        text = text.replace('(', ' ( ').replace(')', ' ) ')\n        text = text.replace('\"', ' \" ').replace(\"'\", \" ' \")\n\n        words = text.split()\n\n        # Convert to IDs\n        token_ids = []\n        for word in words:\n            if word in self.vocab_dict:\n                token_ids.append(self.vocab_dict[word])\n            else:\n                token_ids.append(self.unk_token_id)\n\n        # Pad or truncate\n        if len(token_ids) < max_length:\n            token_ids += [self.pad_token_id] * (max_length - len(token_ids))\n        else:\n            token_ids = token_ids[:max_length]\n\n        return token_ids\n\n    def decode(self, token_ids, skip_special_tokens=True):\n        \"\"\"Decode token IDs to text\"\"\"\n        words = []\n        special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n\n        for idx in token_ids:\n            if isinstance(idx, int):\n                word = self.idx_to_word.get(idx, '<UNK>')\n            else:\n                word = self.idx_to_word.get(idx.item(), '<UNK>')\n\n            if skip_special_tokens and word in special_tokens:\n                continue\n\n            words.append(word)\n\n        # Join with spaces, but handle punctuation\n        text = ' '.join(words)\n        text = text.replace(' ,', ',').replace(' .', '.')\n        text = text.replace('( ', '(').replace(' )', ')')\n\n        return text\n\n# --- Start Embedded Model Definition ---\n\nclass AxisMoE(nn.Module):\n    def __init__(self, d_model=256, n_experts=8, d_axis_emb=128):\n        super().__init__()\n        self.experts = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(n_experts)])\n        self.gate = nn.Linear(d_model + d_axis_emb, n_experts)\n        self.prev_gates = None  # Track previous gates\n\n    def forward(self, h, a):\n        gate_input = torch.cat([h, a.unsqueeze(1).repeat(1, h.size(1), 1)], dim=-1)\n        g = F.softmax(self.gate(gate_input), dim=-1)\n\n        # Sparse routing: top-k experts (k=2)\n        topk_gates, topk_indices = torch.topk(g, k=2, dim=-1)\n        topk_gates = topk_gates / topk_gates.sum(dim=-1, keepdim=True)\n\n        # Simplified and corrected vectorized implementation\n        batch_size, seq_len, d_model = h.shape\n        h_flat = h.view(-1, d_model) # (batch_size * seq_len, d_model)\n\n        final_output = torch.zeros_like(h_flat)\n\n        for i, expert in enumerate(self.experts):\n            # Find which tokens are routed to this expert (top-2)\n            mask = (topk_indices == i).any(dim=-1)\n            mask_flat = mask.view(-1)\n\n            if mask_flat.any():\n                # Get the gate values for the tokens routed to this expert\n                gate_values = torch.where(topk_indices == i, topk_gates, torch.zeros_like(topk_gates)).sum(dim=-1)\n                gate_values_flat = gate_values.view(-1, 1)\n\n                # Apply the expert to the selected tokens and weight by the gate values\n                final_output[mask_flat] += (expert(h_flat[mask_flat]) * gate_values_flat[mask_flat])\n\n        output = final_output.view(batch_size, seq_len, d_model)\n\n        # Compute losses\n        entropy_loss = -(g * torch.log(g + 1e-10)).sum(-1).mean()\n\n        stability_loss = 0.0\n        if self.training and self.prev_gates is not None:\n            stability_loss = F.mse_loss(g, self.prev_gates.detach())\n\n        self.prev_gates = g.detach() if self.training else None\n\n        return output, entropy_loss, stability_loss\n\nclass TrajectorySSM(nn.Module):\n    def __init__(self, d_model, d_state=128, d_axis_emb=128):\n        super().__init__()\n        self.d_state = d_state\n        self.A = nn.Parameter(torch.randn(d_state, d_state))\n        self.B = nn.Linear(d_axis_emb, d_state)\n        self.C = nn.Linear(d_state, d_model)\n        self.h_proj = nn.Linear(d_model, d_state)\n\n    def forward(self, h, a, s_t):\n        h_projected = self.h_proj(h)\n        return torch.tanh(F.linear(s_t, self.A) + self.B(a) * h_projected)\n\nclass AxisAwareGPTWithMoEImproved(nn.Module):\n    def __init__(self, vocab_size, d_model=256, n_axes=4, n_paths_per_axis=3, n_head=8, n_layer=4, d_state=128, n_experts=8):\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_axis_emb = d_model // 2\n\n        self.token_emb = nn.Embedding(vocab_size, d_model)\n        self.axis_emb = nn.Embedding(n_axes * n_paths_per_axis, self.d_axis_emb)\n        self.film_gamma = nn.Linear(self.d_axis_emb, d_model)\n        self.film_beta = nn.Linear(self.d_axis_emb, d_model)\n\n        encoder_layers = nn.TransformerEncoderLayer(d_model, n_head, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layer)\n\n        self.moe_layer = AxisMoE(d_model, n_experts, d_axis_emb=self.d_axis_emb)\n        self.trajectory_ssm = TrajectorySSM(d_model, d_state, self.d_axis_emb)\n        self.trajectory_predictor = nn.Linear(d_state, d_state)\n\n        # Heads\n        self.lm_head = nn.Linear(d_model, vocab_size)\n        self.axis_head = nn.Linear(d_model, n_axes * n_paths_per_axis)\n        self.axis_inference_head = nn.Linear(d_model, n_axes * n_paths_per_axis)\n        self.uncertainty_head = nn.Linear(d_model, 1)\n        self.temp_head = nn.Linear(d_model, 1)\n\n    def get_embeddings(self, tokens):\n        return self.token_emb(tokens)\n\n    def compute_trajectory_loss(self, trajectory_states):\n        predicted_future = self.trajectory_predictor(trajectory_states[:, :-1, :])\n        actual_future = trajectory_states[:, 1:, :].detach()\n        return F.mse_loss(predicted_future, actual_future)\n\n    def forward_from_embeddings(self, embeddings, axis_id):\n        a = self.axis_emb(axis_id)\n        gamma = self.film_gamma(a).unsqueeze(1)\n        beta = self.film_beta(a).unsqueeze(1)\n        x = gamma * embeddings + beta\n\n        h = self.transformer_encoder(x)\n        moe_h, gate_loss, stability_loss = self.moe_layer(h, a)\n        h = h + moe_h\n\n        s_t = torch.zeros(embeddings.size(0), self.d_state).to(embeddings.device)\n        trajectory_states, ssm_outputs = [], []\n        for t in range(embeddings.size(1)):\n            s_t = self.trajectory_ssm(h[:, t, :], a, s_t)\n            trajectory_states.append(s_t)\n            ssm_outputs.append(self.trajectory_ssm.C(s_t))\n\n        trajectory_states = torch.stack(trajectory_states, dim=1)\n\n        combined_h = h + torch.stack(ssm_outputs, dim=1)\n\n        logits = self.lm_head(combined_h)\n        axis_pred = self.axis_head(combined_h)\n        inferred_axis = self.axis_inference_head(combined_h)\n        uncertainty = self.uncertainty_head(combined_h).squeeze(-1)\n        temperature = torch.sigmoid(self.temp_head(combined_h)).squeeze(-1) * 2 + 0.5\n\n        return logits, axis_pred, trajectory_states, gate_loss, stability_loss, uncertainty, temperature, inferred_axis\n\n    def forward(self, tokens, axis_id):\n        embeddings = self.get_embeddings(tokens)\n        return self.forward_from_embeddings(embeddings, axis_id)\n\n    def infer_axis(self, tokens):\n        \"\"\"Infer axis from tokens by marginalizing over all possible axes\"\"\"\n        with torch.no_grad():\n            n_axes = self.axis_emb.num_embeddings\n            all_inferences = []\n\n            for axis_id in range(n_axes):\n                axis_tensor = torch.tensor([axis_id]).to(tokens.device)\n                _, _, _, _, _, _, _, inferred = self.forward(\n                    tokens.unsqueeze(0) if tokens.dim() == 1 else tokens,\n                    axis_tensor\n                )\n                all_inferences.append(inferred)\n\n            avg_inference = torch.stack(all_inferences).mean(dim=0)\n            return torch.argmax(avg_inference.mean(dim=1), dim=1)\n\n# --- End Embedded Model Definition ---\n\nAXIS_NAMES = {\n    0: \"Tech: Hyper-automation\",\n    1: \"Tech: Human-centric\",\n    2: \"Tech: Resource-abundant\",\n    3: \"Society: Individualistic\",\n    4: \"Society: Community\",\n    5: \"Society: Global\",\n    6: \"Environment: Crisis\",\n    7: \"Environment: Restoration\",\n    8: \"Environment: Adaptation\",\n    9: \"Creativity: Immersive/Digital\",\n    10: \"Creativity: Physical/Tangible\",\n    11: \"Creativity: Collaborative\",\n}\n\nclass CustomVocabDataset(Dataset):\n    \"\"\"Dataset using custom vocabulary\"\"\"\n\n    def __init__(self, json_path, tokenizer, max_length=50):\n        with open(json_path) as f:\n            data = json.load(f)\n\n        self.samples = []\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n        # Tokenize all samples\n        for item in data['samples']:\n            text = item['text']\n            axis_id = item['axis_id']\n\n            # Tokenize using custom tokenizer\n            tokens = tokenizer.encode(text, max_length=max_length)\n\n            self.samples.append({\n                'tokens': torch.LongTensor(tokens),\n                'axis_id': axis_id,\n                'text': text\n            })\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        return sample['tokens'], sample['axis_id']\n\n\ndef train_with_custom_vocab(\n    dataset_path=\"futures_dataset.json\",\n    vocab_path=\"custom_vocab.pkl\",\n    d_model=256,\n    num_epochs=40,  # Increased from 20\n    batch_size=16,\n    learning_rate=1e-4,\n    checkpoint_path=\"checkpoint_custom_vocab.pt\"\n):\n    \"\"\"\n    Train with custom vocabulary.\n\n    Key changes from original:\n    1. Uses custom 3K vocab instead of GPT-2's 50K\n    2. Increased axis loss weight (2.0 instead of 1.0)\n    3. Longer training (40 epochs instead of 20)\n    4. Lower initial learning rate for stability\n    \"\"\"\n\n    print(\"=\" * 80)\n    print(\"Training with Custom Vocabulary (FIXED VERSION)\")\n    print(\"=\" * 80)\n\n    # Load custom vocabulary\n    print(\"\\n1. Loading custom vocabulary...\")\n    with open(vocab_path, 'rb') as f:\n        vocab_data = pickle.load(f)\n\n    vocab_dict = vocab_data['vocab_dict']\n    vocab_size = len(vocab_dict)\n\n    print(f\"   Vocabulary size: {vocab_size:,} (was 50,257)\")\n    print(f\"   ✓ Model can handle this!\")\n\n    # Create tokenizer\n    tokenizer = CustomTokenizer(vocab_dict)\n\n    # Load dataset\n    print(f\"\\n2. Loading dataset...\")\n    full_dataset = CustomVocabDataset(dataset_path, tokenizer)\n    print(f\"   Total samples: {len(full_dataset)}\")\n\n    # Split\n    train_size = int(0.8 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = random_split(\n        full_dataset,\n        [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n\n    print(f\"   Train samples: {len(train_dataset)}\")\n    print(f\"   Val samples: {len(val_dataset)}\")\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # Initialize model\n    print(f\"\\n3. Initializing model...\")\n    model = AxisAwareGPTWithMoEImproved(\n        vocab_size=vocab_size,  # KEY FIX: 3K instead of 50K\n        d_model=d_model,\n        n_axes=4,\n        n_paths_per_axis=3,\n        n_head=8,\n        n_layer=4,\n        d_state=128\n    )\n\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"   Total parameters: {total_params:,}\")\n    print(f\"   Parameters per vocab item: {total_params / vocab_size:.0f}\")\n    print(f\"   (Was: {31761139 / 50257:.0f} - not enough capacity!)\")\n\n    # Setup training\n    criterion_nll = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n    criterion_axis = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n\n    from torch.optim.lr_scheduler import OneCycleLR\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=learning_rate * 10,\n        epochs=num_epochs,\n        steps_per_epoch=len(train_loader),\n        pct_start=0.1\n    )\n\n    # Training loop\n    print(f\"\\n4. Training for {num_epochs} epochs...\")\n    print(\"=\" * 80)\n\n    best_val_loss = float('inf')\n    best_axis_acc = 0.0\n\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n\n        for batch_idx, (tokens, axis_ids) in enumerate(train_loader):\n            optimizer.zero_grad()\n\n            # Forward pass\n            logits, axis_pred, traj_states, gate_entropy, gate_stability, _, _, _ = model(tokens, axis_ids)\n\n            # Compute losses\n            loss_nll = criterion_nll(\n                logits.view(-1, vocab_size),\n                tokens.view(-1)\n            )\n            loss_axis = criterion_axis(axis_pred.mean(dim=1), axis_ids)\n            loss_traj = model.compute_trajectory_loss(traj_states)\n\n            # INCREASED AXIS WEIGHT from 1.0 to 2.0\n            loss = (\n                loss_nll +\n                2.0 * loss_axis +      # Was 1.0, now 2.0\n                0.1 * loss_traj +\n                0.01 * gate_entropy +\n                0.05 * gate_stability\n            )\n\n            # Backward\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n\n            train_loss += loss.item()\n\n            # Axis accuracy\n            predicted_axis = axis_pred.mean(dim=1).argmax(dim=1)\n            train_correct += (predicted_axis == axis_ids).sum().item()\n            train_total += axis_ids.size(0)\n\n            # Progress\n            if batch_idx % 10 == 0:\n                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n                      f\"Loss: {loss.item():.4f}, Axis Acc: {100*train_correct/train_total:.1f}%\")\n\n        avg_train_loss = train_loss / len(train_loader)\n        train_axis_acc = 100 * train_correct / train_total\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for tokens, axis_ids in val_loader:\n                logits, axis_pred, _, _, _, _, _, _ = model(tokens, axis_ids)\n\n                loss = criterion_nll(logits.view(-1, vocab_size), tokens.view(-1))\n                val_loss += loss.item()\n\n                predicted_axis = axis_pred.mean(dim=1).argmax(dim=1)\n                val_correct += (predicted_axis == axis_ids).sum().item()\n                val_total += axis_ids.size(0)\n\n        avg_val_loss = val_loss / len(val_loader)\n        val_axis_acc = 100 * val_correct / val_total\n\n        # Calculate perplexity\n        perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n\n        # Epoch summary\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"Epoch {epoch+1}/{num_epochs} Summary\")\n        print(\"=\" * 80)\n        print(f\"Train Loss: {avg_train_loss:.4f} | Train Axis Acc: {train_axis_acc:.2f}%\")\n        print(f\"Val Loss:   {avg_val_loss:.4f} | Val Axis Acc:   {val_axis_acc:.2f}%\")\n        print(f\"Perplexity: {perplexity:.2f}\")\n\n        # Save best model\n        if val_axis_acc > best_axis_acc:\n            best_axis_acc = val_axis_acc\n            best_val_loss = avg_val_loss\n\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': avg_val_loss,\n                'val_axis_acc': val_axis_acc,\n                'train_loss': avg_train_loss,\n                'train_axis_acc': train_axis_acc,\n                'vocab_size': vocab_size,\n                'perplexity': perplexity\n            }, checkpoint_path)\n\n            print(f\"✓ Saved checkpoint (axis acc improved to {val_axis_acc:.2f}%)\")\n\n        print(\"=\" * 80 + \"\\n\")\n\n    # Final summary\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Training Complete!\")\n    print(\"=\" * 80)\n    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n    print(f\"Best Axis Accuracy: {best_axis_acc:.2f}%\")\n    print(f\"Best Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n    print(f\"Checkpoint saved to: {checkpoint_path}\")\n\n    # Interpretation\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Results Interpretation\")\n    print(\"=\" * 80)\n\n    if best_axis_acc >= 75:\n        print(\"✓✓✓ EXCELLENT RESULTS!\")\n        print(\"   Axis accuracy >75% means the model is learning\")\n        print(\"   meaningful semantic differences between futures.\")\n        print(\"\\n   Next steps:\")\n        print(\"   1. Test axis controllability with generation\")\n        print(\"   2. Run ablation study\")\n        print(\"   3. Scale up and write paper!\")\n\n    elif best_axis_acc >= 60:\n        print(\"✓ GOOD RESULTS\")\n        print(\"   Axis accuracy 60-75% shows the model is learning,\")\n        print(\"   but there's room for improvement.\")\n        print(\"\\n   Try:\")\n        print(\"   - Train longer (60-80 epochs)\")\n        print(\"   - Increase model size (d_model=384)\")\n        print(\"   - Increase axis loss weight to 3.0\")\n\n    else:\n        print(\"⚠️  NEEDS IMPROVEMENT\")\n        print(\"   Axis accuracy <60% suggests issues remain.\")\n        print(\"\\n   Check:\")\n        print(\"   - Is perplexity < 30? (language modeling working)\")\n        print(\"   - Are axes separable in the dataset?\")\n        print(\"   - Try even higher axis loss weight (3.0-5.0)\")\n\n    return model, best_axis_acc, tokenizer\n\ndef generate_with_custom_vocab(model, tokenizer, prompt_text, axis_id, max_length=20, temperature=0.8):\n    \"\"\"Generate text with custom vocabulary\"\"\"\n    model.eval()\n\n    # Tokenize prompt\n    prompt_tokens = torch.LongTensor(tokenizer.encode(prompt_text, max_length=50))\n\n    # Find where padding starts\n    pad_id = tokenizer.pad_token_id\n    actual_length = (prompt_tokens != pad_id).sum().item()\n    tokens = prompt_tokens[:actual_length]\n\n    generated_tokens = []\n\n    with torch.no_grad():\n        for _ in range(max_length):\n            # Prepare input (pad to 50 if needed)\n            input_tokens = tokens.clone()\n            if len(input_tokens) < 50:\n                padding = torch.full((50 - len(input_tokens),), pad_id, dtype=torch.long)\n                input_tokens = torch.cat([input_tokens, padding])\n            else:\n                input_tokens = input_tokens[:50]\n\n            # Forward pass\n            logits, _, _, _, _, temp_head, _, _ = model(\n                input_tokens.unsqueeze(0),\n                torch.tensor([axis_id])\n            )\n\n            # Get logits for last real token\n            last_real_pos = min(len(tokens) - 1, 49)\n            next_token_logits = logits[0, last_real_pos, :] / temperature\n            probs = F.softmax(next_token_logits, dim=-1)\n\n            # Sample next token\n            next_token = torch.multinomial(probs, 1)\n\n            # Stop if we generate padding or end token\n            if next_token.item() == pad_id or next_token.item() == tokenizer.vocab_dict.get('<END>', -1):\n                break\n\n            generated_tokens.append(next_token.item())\n            tokens = torch.cat([tokens, next_token])\n\n    # Decode\n    full_tokens = prompt_tokens[:actual_length].tolist() + generated_tokens\n    generated_text = tokenizer.decode(full_tokens, skip_special_tokens=True)\n\n    return generated_text\n\n\n\nif __name__ == \"__main__\":\n    # Define Kaggle paths\n    DATASET_PATH = \"/kaggle/input/futures-dataset/futures_dataset.json\"\n    VOCAB_PATH = \"/kaggle/working/custom_vocab.pkl\"\n    CHECKPOINT_PATH = \"/kaggle/working/checkpoint.pt\"\n\n    # Build vocabulary\n    build_custom_vocabulary(dataset_path=DATASET_PATH)\n\n    # Train the model\n    model, accuracy, tokenizer = train_with_custom_vocab(\n        dataset_path=DATASET_PATH,\n        vocab_path=VOCAB_PATH,\n        checkpoint_path=CHECKPOINT_PATH,\n        d_model=256,\n        num_epochs=40\n    )\n\n    # Test the model\n    if model:\n        test_custom_vocab_controllability(\n            checkpoint_path=CHECKPOINT_PATH,\n            vocab_path=VOCAB_PATH,\n            d_model=256\n        )","metadata":{"_uuid":"d79a8024-bda2-4d2a-a5e8-3c66f54571a3","_cell_guid":"ff7df704-9ca7-42b7-a9d0-c71b2da37216","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:24:40.380985Z","iopub.execute_input":"2025-12-19T06:24:40.381482Z","iopub.status.idle":"2025-12-19T06:36:13.131433Z","shell.execute_reply.started":"2025-12-19T06:24:40.381454Z","shell.execute_reply":"2025-12-19T06:36:13.130495Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nBuilding Custom Vocabulary\n======================================================================\n\n1. Loading dataset from /kaggle/input/futures-dataset/futures_dataset.json...\n   Total samples: 1200\n\n2. Counting words...\n   Total words: 11,270\n   Unique words: 1,083\n\n3. Creating vocabulary (target size: 3000)...\n   Final vocabulary size: 1,087\n   Coverage: 100.00% of dataset tokens\n\n4. Saving vocabulary...\n   Saved to: custom_vocab.json and custom_vocab.pkl\n\n======================================================================\nVocabulary Statistics\n======================================================================\nVocabulary size: 1,087\nCoverage: 100.00%\nUnknown token rate: 0.00%\n\nTop 20 most common words:\n   1. and                  ( 370 occurrences)\n   2. through              ( 260 occurrences)\n   3. to                   ( 185 occurrences)\n   4. using                ( 155 occurrences)\n   5. for                  ( 140 occurrences)\n   6. with                 ( 120 occurrences)\n   7. create               (  95 occurrences)\n   8. on                   (  90 occurrences)\n   9. communities          (  80 occurrences)\n  10. build                (  75 occurrences)\n  11. human                (  70 occurrences)\n  12. ai                   (  70 occurrences)\n  13. develop              (  65 occurrences)\n  14. in                   (  60 occurrences)\n  15. community            (  60 occurrences)\n  16. their                (  55 occurrences)\n  17. shared               (  55 occurrences)\n  18. together             (  50 occurrences)\n  19. programs             (  50 occurrences)\n  20. systems              (  50 occurrences)\n\nSample vocabulary by category:\n\n  Tech words: ai, digital, machine, algorithms, automated, robotic, ai-powered, algorithmic, against, rainfall\n  Environment words: energy, climate, ecosystems, carbon, environmental, sustainable, ecosystem, climate-controlled\n  Society words: community, global, collective, individual, collectively, collectives, globally, individuals, individually, society\n\n======================================================================\n✓ Vocabulary creation complete!\n======================================================================\n\nNext steps:\n  1. Use custom_vocab.pkl in training\n  2. Retrain model with vocab_size = 1087\n  3. Expected: Coherent text generation + 70-80% axis accuracy\n================================================================================\nTraining with Custom Vocabulary (FIXED VERSION)\n================================================================================\n\n1. Loading custom vocabulary...\n   Vocabulary size: 1,087 (was 50,257)\n   ✓ Model can handle this!\n\n2. Loading dataset...\n   Total samples: 1200\n   Train samples: 960\n   Val samples: 240\n\n3. Initializing model...\n   Total parameters: 6,536,929\n   Parameters per vocab item: 6014\n   (Was: 632 - not enough capacity!)\n\n4. Training for 40 epochs...\n================================================================================\nEpoch 1/40, Batch 0/60, Loss: 13.4743, Axis Acc: 6.2%\nEpoch 1/40, Batch 10/60, Loss: 13.0112, Axis Acc: 9.7%\nEpoch 1/40, Batch 20/60, Loss: 13.3631, Axis Acc: 12.2%\nEpoch 1/40, Batch 30/60, Loss: 12.8152, Axis Acc: 11.1%\nEpoch 1/40, Batch 40/60, Loss: 12.7063, Axis Acc: 10.8%\nEpoch 1/40, Batch 50/60, Loss: 13.6485, Axis Acc: 9.7%\n\n================================================================================\nEpoch 1/40 Summary\n================================================================================\nTrain Loss: 13.0751 | Train Axis Acc: 9.06%\nVal Loss:   7.2782 | Val Axis Acc:   5.42%\nPerplexity: 1448.40\n✓ Saved checkpoint (axis acc improved to 5.42%)\n================================================================================\n\nEpoch 2/40, Batch 0/60, Loss: 12.6902, Axis Acc: 6.2%\nEpoch 2/40, Batch 10/60, Loss: 12.8954, Axis Acc: 3.4%\nEpoch 2/40, Batch 20/60, Loss: 12.8373, Axis Acc: 4.2%\nEpoch 2/40, Batch 30/60, Loss: 12.5992, Axis Acc: 4.6%\nEpoch 2/40, Batch 40/60, Loss: 14.5144, Axis Acc: 5.2%\nEpoch 2/40, Batch 50/60, Loss: 13.6093, Axis Acc: 5.9%\n\n================================================================================\nEpoch 2/40 Summary\n================================================================================\nTrain Loss: 13.2681 | Train Axis Acc: 5.94%\nVal Loss:   7.3113 | Val Axis Acc:   10.00%\nPerplexity: 1497.15\n✓ Saved checkpoint (axis acc improved to 10.00%)\n================================================================================\n\nEpoch 3/40, Batch 0/60, Loss: 13.3182, Axis Acc: 12.5%\nEpoch 3/40, Batch 10/60, Loss: 13.0412, Axis Acc: 8.5%\nEpoch 3/40, Batch 20/60, Loss: 13.9399, Axis Acc: 6.2%\nEpoch 3/40, Batch 30/60, Loss: 13.8531, Axis Acc: 8.1%\nEpoch 3/40, Batch 40/60, Loss: 13.9258, Axis Acc: 8.8%\nEpoch 3/40, Batch 50/60, Loss: 13.0793, Axis Acc: 8.6%\n\n================================================================================\nEpoch 3/40 Summary\n================================================================================\nTrain Loss: 13.5064 | Train Axis Acc: 8.75%\nVal Loss:   7.6333 | Val Axis Acc:   9.58%\nPerplexity: 2065.93\n================================================================================\n\nEpoch 4/40, Batch 0/60, Loss: 13.1581, Axis Acc: 6.2%\nEpoch 4/40, Batch 10/60, Loss: 14.4742, Axis Acc: 4.5%\nEpoch 4/40, Batch 20/60, Loss: 13.8433, Axis Acc: 6.5%\nEpoch 4/40, Batch 30/60, Loss: 13.4864, Axis Acc: 6.9%\nEpoch 4/40, Batch 40/60, Loss: 13.6345, Axis Acc: 7.0%\nEpoch 4/40, Batch 50/60, Loss: 13.7024, Axis Acc: 8.0%\n\n================================================================================\nEpoch 4/40 Summary\n================================================================================\nTrain Loss: 14.2056 | Train Axis Acc: 8.02%\nVal Loss:   7.8462 | Val Axis Acc:   9.58%\nPerplexity: 2556.02\n================================================================================\n\nEpoch 5/40, Batch 0/60, Loss: 14.4179, Axis Acc: 6.2%\nEpoch 5/40, Batch 10/60, Loss: 14.4689, Axis Acc: 9.1%\nEpoch 5/40, Batch 20/60, Loss: 13.7076, Axis Acc: 8.6%\nEpoch 5/40, Batch 30/60, Loss: 14.5421, Axis Acc: 8.7%\nEpoch 5/40, Batch 40/60, Loss: 13.4601, Axis Acc: 8.2%\nEpoch 5/40, Batch 50/60, Loss: 13.2516, Axis Acc: 8.9%\n\n================================================================================\nEpoch 5/40 Summary\n================================================================================\nTrain Loss: 14.0647 | Train Axis Acc: 8.02%\nVal Loss:   8.0887 | Val Axis Acc:   9.58%\nPerplexity: 3257.31\n================================================================================\n\nEpoch 6/40, Batch 0/60, Loss: 15.4421, Axis Acc: 0.0%\nEpoch 6/40, Batch 10/60, Loss: 15.0690, Axis Acc: 8.5%\nEpoch 6/40, Batch 20/60, Loss: 14.4601, Axis Acc: 12.8%\nEpoch 6/40, Batch 30/60, Loss: 14.5523, Axis Acc: 16.1%\nEpoch 6/40, Batch 40/60, Loss: 14.8720, Axis Acc: 16.3%\nEpoch 6/40, Batch 50/60, Loss: 14.3316, Axis Acc: 15.9%\n\n================================================================================\nEpoch 6/40 Summary\n================================================================================\nTrain Loss: 14.5518 | Train Axis Acc: 14.58%\nVal Loss:   8.0047 | Val Axis Acc:   8.75%\nPerplexity: 2995.13\n================================================================================\n\nEpoch 7/40, Batch 0/60, Loss: 14.6603, Axis Acc: 6.2%\nEpoch 7/40, Batch 10/60, Loss: 14.8334, Axis Acc: 3.4%\nEpoch 7/40, Batch 20/60, Loss: 14.6195, Axis Acc: 3.9%\nEpoch 7/40, Batch 30/60, Loss: 15.2212, Axis Acc: 2.6%\nEpoch 7/40, Batch 40/60, Loss: 16.1653, Axis Acc: 3.8%\nEpoch 7/40, Batch 50/60, Loss: 16.7876, Axis Acc: 5.3%\n\n================================================================================\nEpoch 7/40 Summary\n================================================================================\nTrain Loss: 15.5941 | Train Axis Acc: 5.21%\nVal Loss:   9.0938 | Val Axis Acc:   5.42%\nPerplexity: 8900.27\n================================================================================\n\nEpoch 8/40, Batch 0/60, Loss: 16.8539, Axis Acc: 6.2%\nEpoch 8/40, Batch 10/60, Loss: 14.9008, Axis Acc: 10.8%\nEpoch 8/40, Batch 20/60, Loss: 15.9082, Axis Acc: 13.1%\nEpoch 8/40, Batch 30/60, Loss: 15.6728, Axis Acc: 15.7%\nEpoch 8/40, Batch 40/60, Loss: 16.0024, Axis Acc: 15.1%\nEpoch 8/40, Batch 50/60, Loss: 15.7404, Axis Acc: 14.0%\n\n================================================================================\nEpoch 8/40 Summary\n================================================================================\nTrain Loss: 15.4686 | Train Axis Acc: 13.23%\nVal Loss:   8.9333 | Val Axis Acc:   5.42%\nPerplexity: 7580.51\n================================================================================\n\nEpoch 9/40, Batch 0/60, Loss: 14.7859, Axis Acc: 0.0%\nEpoch 9/40, Batch 10/60, Loss: 15.9176, Axis Acc: 11.9%\nEpoch 9/40, Batch 20/60, Loss: 14.4131, Axis Acc: 9.5%\nEpoch 9/40, Batch 30/60, Loss: 14.1389, Axis Acc: 8.9%\nEpoch 9/40, Batch 40/60, Loss: 13.4976, Axis Acc: 8.7%\nEpoch 9/40, Batch 50/60, Loss: 13.4030, Axis Acc: 10.5%\n\n================================================================================\nEpoch 9/40 Summary\n================================================================================\nTrain Loss: 14.1016 | Train Axis Acc: 13.96%\nVal Loss:   7.7916 | Val Axis Acc:   29.17%\nPerplexity: 2420.08\n✓ Saved checkpoint (axis acc improved to 29.17%)\n================================================================================\n\nEpoch 10/40, Batch 0/60, Loss: 12.7407, Axis Acc: 25.0%\nEpoch 10/40, Batch 10/60, Loss: 11.4456, Axis Acc: 36.9%\nEpoch 10/40, Batch 20/60, Loss: 11.7850, Axis Acc: 44.3%\nEpoch 10/40, Batch 30/60, Loss: 12.5210, Axis Acc: 44.6%\nEpoch 10/40, Batch 40/60, Loss: 12.5913, Axis Acc: 38.0%\nEpoch 10/40, Batch 50/60, Loss: 13.2904, Axis Acc: 36.0%\n\n================================================================================\nEpoch 10/40 Summary\n================================================================================\nTrain Loss: 12.3183 | Train Axis Acc: 32.19%\nVal Loss:   8.0992 | Val Axis Acc:   10.00%\nPerplexity: 3291.97\n================================================================================\n\nEpoch 11/40, Batch 0/60, Loss: 14.7306, Axis Acc: 18.8%\nEpoch 11/40, Batch 10/60, Loss: 14.5310, Axis Acc: 12.5%\nEpoch 11/40, Batch 20/60, Loss: 17.6679, Axis Acc: 9.5%\nEpoch 11/40, Batch 30/60, Loss: 18.9182, Axis Acc: 6.5%\nEpoch 11/40, Batch 40/60, Loss: 17.4882, Axis Acc: 4.9%\nEpoch 11/40, Batch 50/60, Loss: 21.5422, Axis Acc: 5.8%\n\n================================================================================\nEpoch 11/40 Summary\n================================================================================\nTrain Loss: 17.6388 | Train Axis Acc: 5.73%\nVal Loss:   9.4674 | Val Axis Acc:   9.58%\nPerplexity: 12931.16\n================================================================================\n\nEpoch 12/40, Batch 0/60, Loss: 21.0458, Axis Acc: 0.0%\nEpoch 12/40, Batch 10/60, Loss: 18.9479, Axis Acc: 8.0%\nEpoch 12/40, Batch 20/60, Loss: 17.3215, Axis Acc: 6.0%\nEpoch 12/40, Batch 30/60, Loss: 18.0936, Axis Acc: 4.0%\nEpoch 12/40, Batch 40/60, Loss: 17.8408, Axis Acc: 4.9%\nEpoch 12/40, Batch 50/60, Loss: 15.9424, Axis Acc: 3.9%\n\n================================================================================\nEpoch 12/40 Summary\n================================================================================\nTrain Loss: 17.9597 | Train Axis Acc: 4.27%\nVal Loss:   9.0001 | Val Axis Acc:   5.00%\nPerplexity: 8103.83\n================================================================================\n\nEpoch 13/40, Batch 0/60, Loss: 15.2660, Axis Acc: 12.5%\nEpoch 13/40, Batch 10/60, Loss: 13.5564, Axis Acc: 10.8%\nEpoch 13/40, Batch 20/60, Loss: 16.5263, Axis Acc: 12.5%\nEpoch 13/40, Batch 30/60, Loss: 18.1512, Axis Acc: 14.9%\nEpoch 13/40, Batch 40/60, Loss: 18.8743, Axis Acc: 14.0%\nEpoch 13/40, Batch 50/60, Loss: 16.2375, Axis Acc: 15.2%\n\n================================================================================\nEpoch 13/40 Summary\n================================================================================\nTrain Loss: 16.9105 | Train Axis Acc: 16.35%\nVal Loss:   10.5929 | Val Axis Acc:   24.58%\nPerplexity: 39848.93\n================================================================================\n\nEpoch 14/40, Batch 0/60, Loss: 17.6504, Axis Acc: 12.5%\nEpoch 14/40, Batch 10/60, Loss: 16.6333, Axis Acc: 18.2%\nEpoch 14/40, Batch 20/60, Loss: 17.5828, Axis Acc: 17.6%\nEpoch 14/40, Batch 30/60, Loss: 17.5283, Axis Acc: 17.5%\nEpoch 14/40, Batch 40/60, Loss: 17.1430, Axis Acc: 16.5%\nEpoch 14/40, Batch 50/60, Loss: 15.2218, Axis Acc: 14.7%\n\n================================================================================\nEpoch 14/40 Summary\n================================================================================\nTrain Loss: 17.0015 | Train Axis Acc: 15.10%\nVal Loss:   7.9925 | Val Axis Acc:   23.75%\nPerplexity: 2958.67\n================================================================================\n\nEpoch 15/40, Batch 0/60, Loss: 15.5566, Axis Acc: 25.0%\nEpoch 15/40, Batch 10/60, Loss: 16.7507, Axis Acc: 23.3%\nEpoch 15/40, Batch 20/60, Loss: 20.6204, Axis Acc: 28.3%\nEpoch 15/40, Batch 30/60, Loss: 14.9825, Axis Acc: 38.1%\nEpoch 15/40, Batch 40/60, Loss: 14.1801, Axis Acc: 48.8%\nEpoch 15/40, Batch 50/60, Loss: 21.4722, Axis Acc: 55.0%\n\n================================================================================\nEpoch 15/40 Summary\n================================================================================\nTrain Loss: 16.3964 | Train Axis Acc: 60.00%\nVal Loss:   11.2381 | Val Axis Acc:   100.00%\nPerplexity: 75972.65\n✓ Saved checkpoint (axis acc improved to 100.00%)\n================================================================================\n\nEpoch 16/40, Batch 0/60, Loss: 10.9019, Axis Acc: 100.0%\nEpoch 16/40, Batch 10/60, Loss: 12.3936, Axis Acc: 95.5%\nEpoch 16/40, Batch 20/60, Loss: 14.5307, Axis Acc: 94.3%\nEpoch 16/40, Batch 30/60, Loss: 12.8968, Axis Acc: 93.8%\nEpoch 16/40, Batch 40/60, Loss: 12.9939, Axis Acc: 93.0%\nEpoch 16/40, Batch 50/60, Loss: 13.3662, Axis Acc: 92.5%\n\n================================================================================\nEpoch 16/40 Summary\n================================================================================\nTrain Loss: 13.4616 | Train Axis Acc: 91.35%\nVal Loss:   16.2140 | Val Axis Acc:   84.58%\nPerplexity: 11006671.00\n================================================================================\n\nEpoch 17/40, Batch 0/60, Loss: 16.1227, Axis Acc: 81.2%\nEpoch 17/40, Batch 10/60, Loss: 16.4023, Axis Acc: 83.5%\nEpoch 17/40, Batch 20/60, Loss: 18.2560, Axis Acc: 82.7%\nEpoch 17/40, Batch 30/60, Loss: 13.2999, Axis Acc: 84.9%\nEpoch 17/40, Batch 40/60, Loss: 12.9814, Axis Acc: 88.4%\nEpoch 17/40, Batch 50/60, Loss: 10.3792, Axis Acc: 90.7%\n\n================================================================================\nEpoch 17/40 Summary\n================================================================================\nTrain Loss: 13.4064 | Train Axis Acc: 92.08%\nVal Loss:   8.1856 | Val Axis Acc:   100.00%\nPerplexity: 3588.91\n================================================================================\n\nEpoch 18/40, Batch 0/60, Loss: 7.3543, Axis Acc: 100.0%\nEpoch 18/40, Batch 10/60, Loss: 11.3812, Axis Acc: 100.0%\nEpoch 18/40, Batch 20/60, Loss: 11.7901, Axis Acc: 100.0%\nEpoch 18/40, Batch 30/60, Loss: 7.9258, Axis Acc: 100.0%\nEpoch 18/40, Batch 40/60, Loss: 9.1358, Axis Acc: 100.0%\nEpoch 18/40, Batch 50/60, Loss: 8.7079, Axis Acc: 100.0%\n\n================================================================================\nEpoch 18/40 Summary\n================================================================================\nTrain Loss: 9.9162 | Train Axis Acc: 100.00%\nVal Loss:   10.1009 | Val Axis Acc:   100.00%\nPerplexity: 24363.83\n================================================================================\n\nEpoch 19/40, Batch 0/60, Loss: 9.8810, Axis Acc: 100.0%\nEpoch 19/40, Batch 10/60, Loss: 9.9445, Axis Acc: 94.9%\nEpoch 19/40, Batch 20/60, Loss: 11.0326, Axis Acc: 94.6%\nEpoch 19/40, Batch 30/60, Loss: 9.8008, Axis Acc: 96.4%\nEpoch 19/40, Batch 40/60, Loss: 10.5167, Axis Acc: 97.3%\nEpoch 19/40, Batch 50/60, Loss: 10.8285, Axis Acc: 97.8%\n\n================================================================================\nEpoch 19/40 Summary\n================================================================================\nTrain Loss: 10.2533 | Train Axis Acc: 97.60%\nVal Loss:   10.7563 | Val Axis Acc:   92.08%\nPerplexity: 46924.72\n================================================================================\n\nEpoch 20/40, Batch 0/60, Loss: 10.5106, Axis Acc: 93.8%\nEpoch 20/40, Batch 10/60, Loss: 10.8614, Axis Acc: 94.9%\nEpoch 20/40, Batch 20/60, Loss: 11.0132, Axis Acc: 97.3%\nEpoch 20/40, Batch 30/60, Loss: 9.8284, Axis Acc: 98.2%\nEpoch 20/40, Batch 40/60, Loss: 10.3418, Axis Acc: 98.6%\nEpoch 20/40, Batch 50/60, Loss: 11.4576, Axis Acc: 98.9%\n\n================================================================================\nEpoch 20/40 Summary\n================================================================================\nTrain Loss: 10.5154 | Train Axis Acc: 99.06%\nVal Loss:   11.4587 | Val Axis Acc:   100.00%\nPerplexity: 94726.56\n================================================================================\n\nEpoch 21/40, Batch 0/60, Loss: 10.4506, Axis Acc: 100.0%\nEpoch 21/40, Batch 10/60, Loss: 10.8417, Axis Acc: 100.0%\nEpoch 21/40, Batch 20/60, Loss: 11.4857, Axis Acc: 100.0%\nEpoch 21/40, Batch 30/60, Loss: 9.9483, Axis Acc: 100.0%\nEpoch 21/40, Batch 40/60, Loss: 8.2300, Axis Acc: 100.0%\nEpoch 21/40, Batch 50/60, Loss: 7.8195, Axis Acc: 100.0%\n\n================================================================================\nEpoch 21/40 Summary\n================================================================================\nTrain Loss: 9.5982 | Train Axis Acc: 100.00%\nVal Loss:   7.6199 | Val Axis Acc:   100.00%\nPerplexity: 2038.27\n================================================================================\n\nEpoch 22/40, Batch 0/60, Loss: 6.5485, Axis Acc: 100.0%\nEpoch 22/40, Batch 10/60, Loss: 7.4383, Axis Acc: 100.0%\nEpoch 22/40, Batch 20/60, Loss: 7.2877, Axis Acc: 100.0%\nEpoch 22/40, Batch 30/60, Loss: 8.0033, Axis Acc: 100.0%\nEpoch 22/40, Batch 40/60, Loss: 7.5742, Axis Acc: 100.0%\nEpoch 22/40, Batch 50/60, Loss: 8.0900, Axis Acc: 98.3%\n\n================================================================================\nEpoch 22/40 Summary\n================================================================================\nTrain Loss: 7.5908 | Train Axis Acc: 98.54%\nVal Loss:   7.7453 | Val Axis Acc:   100.00%\nPerplexity: 2310.71\n================================================================================\n\nEpoch 23/40, Batch 0/60, Loss: 7.0172, Axis Acc: 100.0%\nEpoch 23/40, Batch 10/60, Loss: 7.3072, Axis Acc: 100.0%\nEpoch 23/40, Batch 20/60, Loss: 7.7600, Axis Acc: 98.2%\nEpoch 23/40, Batch 30/60, Loss: 12.7362, Axis Acc: 85.3%\nEpoch 23/40, Batch 40/60, Loss: 15.3674, Axis Acc: 78.4%\nEpoch 23/40, Batch 50/60, Loss: 22.1556, Axis Acc: 65.4%\n\n================================================================================\nEpoch 23/40 Summary\n================================================================================\nTrain Loss: 13.0834 | Train Axis Acc: 55.73%\nVal Loss:   10.1195 | Val Axis Acc:   0.00%\nPerplexity: 24821.91\n================================================================================\n\nEpoch 24/40, Batch 0/60, Loss: 25.5441, Axis Acc: 0.0%\nEpoch 24/40, Batch 10/60, Loss: 27.9282, Axis Acc: 0.0%\nEpoch 24/40, Batch 20/60, Loss: 30.4969, Axis Acc: 0.0%\nEpoch 24/40, Batch 30/60, Loss: 29.7383, Axis Acc: 0.0%\nEpoch 24/40, Batch 40/60, Loss: 24.5974, Axis Acc: 0.0%\nEpoch 24/40, Batch 50/60, Loss: 29.6350, Axis Acc: 0.0%\n\n================================================================================\nEpoch 24/40 Summary\n================================================================================\nTrain Loss: 28.1932 | Train Axis Acc: 0.00%\nVal Loss:   10.6486 | Val Axis Acc:   0.00%\nPerplexity: 42131.62\n================================================================================\n\nEpoch 25/40, Batch 0/60, Loss: 32.0513, Axis Acc: 0.0%\nEpoch 25/40, Batch 10/60, Loss: 28.3000, Axis Acc: 0.0%\nEpoch 25/40, Batch 20/60, Loss: 33.6479, Axis Acc: 0.0%\nEpoch 25/40, Batch 30/60, Loss: 27.3800, Axis Acc: 0.0%\nEpoch 25/40, Batch 40/60, Loss: 30.0521, Axis Acc: 0.0%\nEpoch 25/40, Batch 50/60, Loss: 29.4160, Axis Acc: 0.0%\n\n================================================================================\nEpoch 25/40 Summary\n================================================================================\nTrain Loss: 30.3528 | Train Axis Acc: 0.00%\nVal Loss:   11.1099 | Val Axis Acc:   0.00%\nPerplexity: 66830.43\n================================================================================\n\nEpoch 26/40, Batch 0/60, Loss: 27.9084, Axis Acc: 0.0%\nEpoch 26/40, Batch 10/60, Loss: 32.3355, Axis Acc: 0.0%\nEpoch 26/40, Batch 20/60, Loss: 27.2104, Axis Acc: 0.0%\nEpoch 26/40, Batch 30/60, Loss: 31.7983, Axis Acc: 0.0%\nEpoch 26/40, Batch 40/60, Loss: 26.5756, Axis Acc: 0.0%\nEpoch 26/40, Batch 50/60, Loss: 31.3347, Axis Acc: 0.0%\n\n================================================================================\nEpoch 26/40 Summary\n================================================================================\nTrain Loss: 29.4353 | Train Axis Acc: 0.00%\nVal Loss:   11.4371 | Val Axis Acc:   0.00%\nPerplexity: 92700.58\n================================================================================\n\nEpoch 27/40, Batch 0/60, Loss: 28.2787, Axis Acc: 0.0%\nEpoch 27/40, Batch 10/60, Loss: 27.6398, Axis Acc: 0.0%\nEpoch 27/40, Batch 20/60, Loss: 34.9815, Axis Acc: 0.0%\nEpoch 27/40, Batch 30/60, Loss: 32.5788, Axis Acc: 0.0%\nEpoch 27/40, Batch 40/60, Loss: 31.0761, Axis Acc: 0.0%\nEpoch 27/40, Batch 50/60, Loss: 28.1002, Axis Acc: 1.6%\n\n================================================================================\nEpoch 27/40 Summary\n================================================================================\nTrain Loss: 30.4041 | Train Axis Acc: 2.08%\nVal Loss:   11.3138 | Val Axis Acc:   10.00%\nPerplexity: 81945.24\n================================================================================\n\nEpoch 28/40, Batch 0/60, Loss: 34.7306, Axis Acc: 6.2%\nEpoch 28/40, Batch 10/60, Loss: 34.9333, Axis Acc: 4.5%\nEpoch 28/40, Batch 20/60, Loss: 29.1333, Axis Acc: 5.1%\nEpoch 28/40, Batch 30/60, Loss: 32.9666, Axis Acc: 5.4%\nEpoch 28/40, Batch 40/60, Loss: 30.3596, Axis Acc: 7.0%\nEpoch 28/40, Batch 50/60, Loss: 28.1454, Axis Acc: 8.2%\n\n================================================================================\nEpoch 28/40 Summary\n================================================================================\nTrain Loss: 29.9926 | Train Axis Acc: 7.92%\nVal Loss:   11.1602 | Val Axis Acc:   10.00%\nPerplexity: 70276.02\n================================================================================\n\nEpoch 29/40, Batch 0/60, Loss: 32.5924, Axis Acc: 0.0%\nEpoch 29/40, Batch 10/60, Loss: 28.4311, Axis Acc: 5.7%\nEpoch 29/40, Batch 20/60, Loss: 28.9899, Axis Acc: 7.4%\nEpoch 29/40, Batch 30/60, Loss: 32.7609, Axis Acc: 7.5%\nEpoch 29/40, Batch 40/60, Loss: 27.9795, Axis Acc: 8.1%\nEpoch 29/40, Batch 50/60, Loss: 30.1768, Axis Acc: 7.5%\n\n================================================================================\nEpoch 29/40 Summary\n================================================================================\nTrain Loss: 29.1226 | Train Axis Acc: 6.77%\nVal Loss:   11.0821 | Val Axis Acc:   2.08%\nPerplexity: 64994.19\n================================================================================\n\nEpoch 30/40, Batch 0/60, Loss: 29.5858, Axis Acc: 0.0%\nEpoch 30/40, Batch 10/60, Loss: 30.1221, Axis Acc: 2.8%\nEpoch 30/40, Batch 20/60, Loss: 29.7963, Axis Acc: 2.1%\nEpoch 30/40, Batch 30/60, Loss: 30.7826, Axis Acc: 2.8%\nEpoch 30/40, Batch 40/60, Loss: 28.0145, Axis Acc: 3.2%\nEpoch 30/40, Batch 50/60, Loss: 26.7632, Axis Acc: 3.9%\n\n================================================================================\nEpoch 30/40 Summary\n================================================================================\nTrain Loss: 27.7607 | Train Axis Acc: 5.21%\nVal Loss:   10.7285 | Val Axis Acc:   10.00%\nPerplexity: 45637.38\n================================================================================\n\nEpoch 31/40, Batch 0/60, Loss: 26.2969, Axis Acc: 6.2%\nEpoch 31/40, Batch 10/60, Loss: 28.3564, Axis Acc: 9.1%\nEpoch 31/40, Batch 20/60, Loss: 28.0006, Axis Acc: 7.4%\nEpoch 31/40, Batch 30/60, Loss: 29.4431, Axis Acc: 8.5%\nEpoch 31/40, Batch 40/60, Loss: 26.0756, Axis Acc: 8.5%\nEpoch 31/40, Batch 50/60, Loss: 28.5718, Axis Acc: 8.3%\n\n================================================================================\nEpoch 31/40 Summary\n================================================================================\nTrain Loss: 27.3891 | Train Axis Acc: 7.92%\nVal Loss:   10.7604 | Val Axis Acc:   10.00%\nPerplexity: 47117.23\n================================================================================\n\nEpoch 32/40, Batch 0/60, Loss: 27.4643, Axis Acc: 6.2%\nEpoch 32/40, Batch 10/60, Loss: 28.8243, Axis Acc: 7.4%\nEpoch 32/40, Batch 20/60, Loss: 23.9448, Axis Acc: 9.2%\nEpoch 32/40, Batch 30/60, Loss: 30.6438, Axis Acc: 9.5%\nEpoch 32/40, Batch 40/60, Loss: 27.9752, Axis Acc: 8.8%\nEpoch 32/40, Batch 50/60, Loss: 24.8698, Axis Acc: 8.0%\n\n================================================================================\nEpoch 32/40 Summary\n================================================================================\nTrain Loss: 26.9406 | Train Axis Acc: 7.92%\nVal Loss:   10.9319 | Val Axis Acc:   10.00%\nPerplexity: 55935.12\n================================================================================\n\nEpoch 33/40, Batch 0/60, Loss: 28.7824, Axis Acc: 6.2%\nEpoch 33/40, Batch 10/60, Loss: 25.4104, Axis Acc: 7.4%\nEpoch 33/40, Batch 20/60, Loss: 22.7117, Axis Acc: 7.7%\nEpoch 33/40, Batch 30/60, Loss: 25.3060, Axis Acc: 7.3%\nEpoch 33/40, Batch 40/60, Loss: 24.2162, Axis Acc: 7.9%\nEpoch 33/40, Batch 50/60, Loss: 26.6118, Axis Acc: 8.2%\n\n================================================================================\nEpoch 33/40 Summary\n================================================================================\nTrain Loss: 26.6711 | Train Axis Acc: 7.92%\nVal Loss:   10.8432 | Val Axis Acc:   10.00%\nPerplexity: 51186.50\n================================================================================\n\nEpoch 34/40, Batch 0/60, Loss: 29.3856, Axis Acc: 6.2%\nEpoch 34/40, Batch 10/60, Loss: 26.0239, Axis Acc: 6.8%\nEpoch 34/40, Batch 20/60, Loss: 24.2589, Axis Acc: 8.6%\nEpoch 34/40, Batch 30/60, Loss: 26.7248, Axis Acc: 9.3%\nEpoch 34/40, Batch 40/60, Loss: 28.8029, Axis Acc: 8.2%\nEpoch 34/40, Batch 50/60, Loss: 28.4715, Axis Acc: 8.5%\n\n================================================================================\nEpoch 34/40 Summary\n================================================================================\nTrain Loss: 26.1850 | Train Axis Acc: 8.12%\nVal Loss:   10.9018 | Val Axis Acc:   10.00%\nPerplexity: 54271.96\n================================================================================\n\nEpoch 35/40, Batch 0/60, Loss: 25.5843, Axis Acc: 12.5%\nEpoch 35/40, Batch 10/60, Loss: 27.8975, Axis Acc: 6.8%\nEpoch 35/40, Batch 20/60, Loss: 28.1190, Axis Acc: 7.1%\nEpoch 35/40, Batch 30/60, Loss: 27.2641, Axis Acc: 8.3%\nEpoch 35/40, Batch 40/60, Loss: 24.3455, Axis Acc: 7.5%\nEpoch 35/40, Batch 50/60, Loss: 25.2883, Axis Acc: 7.5%\n\n================================================================================\nEpoch 35/40 Summary\n================================================================================\nTrain Loss: 26.5655 | Train Axis Acc: 7.92%\nVal Loss:   10.9136 | Val Axis Acc:   10.00%\nPerplexity: 54917.72\n================================================================================\n\nEpoch 36/40, Batch 0/60, Loss: 29.1349, Axis Acc: 0.0%\nEpoch 36/40, Batch 10/60, Loss: 26.2663, Axis Acc: 9.1%\nEpoch 36/40, Batch 20/60, Loss: 29.5715, Axis Acc: 7.4%\nEpoch 36/40, Batch 30/60, Loss: 24.1874, Axis Acc: 7.5%\nEpoch 36/40, Batch 40/60, Loss: 28.0472, Axis Acc: 7.5%\nEpoch 36/40, Batch 50/60, Loss: 25.4025, Axis Acc: 7.7%\n\n================================================================================\nEpoch 36/40 Summary\n================================================================================\nTrain Loss: 26.3840 | Train Axis Acc: 7.92%\nVal Loss:   10.9512 | Val Axis Acc:   10.00%\nPerplexity: 57020.66\n================================================================================\n\nEpoch 37/40, Batch 0/60, Loss: 24.1886, Axis Acc: 6.2%\nEpoch 37/40, Batch 10/60, Loss: 28.7749, Axis Acc: 7.4%\nEpoch 37/40, Batch 20/60, Loss: 23.7759, Axis Acc: 6.5%\nEpoch 37/40, Batch 30/60, Loss: 23.5299, Axis Acc: 7.5%\nEpoch 37/40, Batch 40/60, Loss: 24.4837, Axis Acc: 7.9%\nEpoch 37/40, Batch 50/60, Loss: 28.4401, Axis Acc: 8.1%\n\n================================================================================\nEpoch 37/40 Summary\n================================================================================\nTrain Loss: 26.3203 | Train Axis Acc: 7.92%\nVal Loss:   10.9163 | Val Axis Acc:   10.00%\nPerplexity: 55064.62\n================================================================================\n\nEpoch 38/40, Batch 0/60, Loss: 28.2843, Axis Acc: 6.2%\nEpoch 38/40, Batch 10/60, Loss: 29.0573, Axis Acc: 6.8%\nEpoch 38/40, Batch 20/60, Loss: 25.1605, Axis Acc: 7.7%\nEpoch 38/40, Batch 30/60, Loss: 28.4378, Axis Acc: 7.7%\nEpoch 38/40, Batch 40/60, Loss: 24.8429, Axis Acc: 8.5%\nEpoch 38/40, Batch 50/60, Loss: 28.9580, Axis Acc: 8.0%\n\n================================================================================\nEpoch 38/40 Summary\n================================================================================\nTrain Loss: 26.2119 | Train Axis Acc: 7.92%\nVal Loss:   10.9118 | Val Axis Acc:   10.00%\nPerplexity: 54817.57\n================================================================================\n\nEpoch 39/40, Batch 0/60, Loss: 25.2117, Axis Acc: 6.2%\nEpoch 39/40, Batch 10/60, Loss: 23.0814, Axis Acc: 6.2%\nEpoch 39/40, Batch 20/60, Loss: 23.5300, Axis Acc: 7.4%\nEpoch 39/40, Batch 30/60, Loss: 25.3770, Axis Acc: 8.3%\nEpoch 39/40, Batch 40/60, Loss: 28.3886, Axis Acc: 9.1%\nEpoch 39/40, Batch 50/60, Loss: 27.4649, Axis Acc: 8.5%\n\n================================================================================\nEpoch 39/40 Summary\n================================================================================\nTrain Loss: 26.1855 | Train Axis Acc: 7.92%\nVal Loss:   10.9149 | Val Axis Acc:   10.00%\nPerplexity: 54988.11\n================================================================================\n\nEpoch 40/40, Batch 0/60, Loss: 26.0660, Axis Acc: 12.5%\nEpoch 40/40, Batch 10/60, Loss: 24.3135, Axis Acc: 5.1%\nEpoch 40/40, Batch 20/60, Loss: 20.7903, Axis Acc: 6.2%\nEpoch 40/40, Batch 30/60, Loss: 25.8008, Axis Acc: 6.7%\nEpoch 40/40, Batch 40/60, Loss: 25.8542, Axis Acc: 7.0%\nEpoch 40/40, Batch 50/60, Loss: 26.8235, Axis Acc: 8.0%\n\n================================================================================\nEpoch 40/40 Summary\n================================================================================\nTrain Loss: 26.1871 | Train Axis Acc: 7.92%\nVal Loss:   10.9159 | Val Axis Acc:   10.00%\nPerplexity: 55046.61\n================================================================================\n\n\n================================================================================\nTraining Complete!\n================================================================================\nBest Val Loss: 11.2381\nBest Axis Accuracy: 100.00%\nBest Perplexity: 75972.65\nCheckpoint saved to: /kaggle/working/checkpoint.pt\n\n================================================================================\nResults Interpretation\n================================================================================\n✓✓✓ EXCELLENT RESULTS!\n   Axis accuracy >75% means the model is learning\n   meaningful semantic differences between futures.\n\n   Next steps:\n   1. Test axis controllability with generation\n   2. Run ablation study\n   3. Scale up and write paper!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/409196703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;31m# Test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         test_custom_vocab_controllability(\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0mvocab_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_custom_vocab_controllability' is not defined"],"ename":"NameError","evalue":"name 'test_custom_vocab_controllability' is not defined","output_type":"error"}],"execution_count":2}]}